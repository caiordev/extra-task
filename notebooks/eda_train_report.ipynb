{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Detection â€“ Shared LSTM (Option B)\n",
    "This notebook reproduces the full pipeline required in **description.pdf**: EDA, window generation, model training, evaluation (confusion matrix, ROC, PR) and conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install extra dependencies inside notebook if running on Colab\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, itertools, pathlib, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Append project root to path so we can `import codes.*` when running notebook directly\n",
    "PROJ_ROOT = pathlib.Path('..').resolve()\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "\n",
    "from codes.data_utils import load_csv_parts, window_generator, train_val_test_split\n",
    "from codes.TensorflowDataPreprocessor import TensorflowDataPreprocessor\n",
    "from codes.LSTM import LSTMModel, F1Score\n",
    "from codes.ModelEvaluator import ModelEvaluator\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = PROJ_ROOT / 'dataset'\n",
    "df_raw = load_csv_parts(DATA_DIR)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick EDA: label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df_raw['label'].value_counts()\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Label distribution (sensor failures)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Window generation & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 240\n",
    "STEP_SIZE = 1\n",
    "X, y = window_generator(df_raw, window_size=WINDOW_SIZE, step_size=STEP_SIZE, use_diff_mean=True)\n",
    "print('Total windows:', len(X), 'Positive:', y.sum())\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalization & tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = TensorflowDataPreprocessor()\n",
    "X_train_e, X_val_e, X_test_e = prep.normalize_3way(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall'), F1Score()]\n",
    "model_wrap = LSTMModel(window_size=WINDOW_SIZE, metrics=metrics)\n",
    "callbacks = model_wrap.setup_callbacks(model_name='notebook')\n",
    "history = model_wrap.model.fit(\n        X_train_e, y_train,\n        validation_data=(X_val_e, y_val),\n        epochs=100, batch_size=128, shuffle=True,\n        class_weight=prep.compute_class_weights(y_train),\n        callbacks=callbacks\n    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_test = model_wrap.model.predict(X_test_e).ravel()\n",
    "evaluator = ModelEvaluator(probs_test, y_test, threshold=-1, minPrecision=0.7)\n",
    "evaluator.execute()\n",
    "evaluator.printMetrics(evaluator.metrics)\n",
    "# Confusion matrix\n",
    "preds = (probs_test >= evaluator.estimatedThreshold).astype(int)\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "# Plot ROC and PR curves\n",
    "evaluator.plotCurve(evaluator.ROCCurve['falsePositiveRates'], evaluator.ROCCurve['truePositiveRates'], evaluator.ROCCurve['AUROC'], 'FPR', 'TPR', 'ROC Curve')\n",
    "evaluator.plotCurve(evaluator.PRCurve['recalls'], evaluator.PRCurve['precisions'], evaluator.PRCurve['AUPRC'], 'Recall', 'Precision', 'PR Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion\n",
    "Summarize results, limitations and potential improvements (e.g., attention model, spectrogram CNN)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
